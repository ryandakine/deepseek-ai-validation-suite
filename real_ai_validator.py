#!/usr/bin/env python3
"""
REAL AI VALIDATION - NO SIMULATIONS
===================================

Actually calls real AI APIs for code validation.
No fake simulated results - real AI analysis!
"""

import asyncio
import openai
import os
import json
import re
from typing import Dict, List, Optional

class RealAIValidator:
    """Real AI code validation using actual APIs"""
    
    def __init__(self):
        # Try to get API keys from environment
        self.openai_key = os.getenv('OPENAI_API_KEY')
        self.anthropic_key = os.getenv('ANTHROPIC_API_KEY')
        
        if self.openai_key:
            openai.api_key = self.openai_key
    
    async def validate_with_openai(self, code: str) -> Dict:
        """Real OpenAI GPT validation"""
        if not self.openai_key:
            return {"error": "No OpenAI API key"}
        
        try:
            prompt = f"""
Analyze this code for security vulnerabilities and rate the risk from 0.0 to 1.0:

```
{code}
```

Return a JSON response with:
- risk_score (0.0-1.0)
- issues (list of security issues found)
- confidence (0.0-1.0)

Format: {{"risk_score": 0.0, "issues": [], "confidence": 0.0}}
"""
            
            response = openai.ChatCompletion.create(
                model="gpt-3.5-turbo",
                messages=[{"role": "user", "content": prompt}],
                max_tokens=500,
                temperature=0.1
            )
            
            content = response.choices[0].message.content.strip()
            
            # Try to extract JSON from response
            try:
                # Find JSON in the response
                json_match = re.search(r'\{.*\}', content, re.DOTALL)
                if json_match:
                    result = json.loads(json_match.group())
                    return {
                        "agent": "GPT-3.5-Turbo",
                        "risk_score": float(result.get("risk_score", 0.0)),
                        "issues": result.get("issues", []),
                        "confidence": float(result.get("confidence", 0.8)),
                        "raw_response": content
                    }
            except:
                pass
            
            # Fallback parsing
            risk_score = self._extract_risk_score(content)
            issues = self._extract_issues(content)
            
            return {
                "agent": "GPT-3.5-Turbo",
                "risk_score": risk_score,
                "issues": issues,
                "confidence": 0.8,
                "raw_response": content
            }
            
        except Exception as e:
            return {"agent": "GPT-3.5-Turbo", "error": str(e)}
    
    def _extract_risk_score(self, text: str) -> float:
        """Extract risk score from AI response"""
        # Look for patterns like "risk: 0.8" or "score: 0.7"
        patterns = [
            r'risk[_\s]*score[:\s]*([0-9.]+)',
            r'risk[:\s]*([0-9.]+)',
            r'score[:\s]*([0-9.]+)',
        ]
        
        for pattern in patterns:
            match = re.search(pattern, text, re.IGNORECASE)
            if match:
                try:
                    score = float(match.group(1))
                    return min(max(score, 0.0), 1.0)  # Clamp to 0-1
                except:
                    continue
        
        # Count security keywords as fallback
        security_keywords = [
            'sql injection', 'xss', 'vulnerability', 'security risk',
            'dangerous', 'exploit', 'attack', 'malicious', 'unsafe'
        ]
        
        count = sum(1 for keyword in security_keywords if keyword in text.lower())
        return min(count * 0.2, 1.0)
    
    def _extract_issues(self, text: str) -> List[str]:
        """Extract security issues from AI response"""
        issues = []
        
        # Look for common security patterns
        if 'sql injection' in text.lower():
            issues.append('Potential SQL injection vulnerability')
        if 'xss' in text.lower() or 'cross-site scripting' in text.lower():
            issues.append('Cross-site scripting (XSS) risk')
        if 'hardcoded' in text.lower() and ('password' in text.lower() or 'key' in text.lower()):
            issues.append('Hardcoded credentials detected')
        if 'eval(' in text or 'exec(' in text:
            issues.append('Code injection risk via eval/exec')
        if 'os.system' in text or 'subprocess' in text:
            issues.append('Command injection vulnerability')
        
        return issues[:5]  # Limit to top 5 issues
    
    async def quick_validate(self, code: str) -> Dict:
        """Quick validation using pattern matching + AI if available"""
        result = {
            "agent": "Pattern Matcher + AI",
            "risk_score": 0.0,
            "issues": [],
            "confidence": 0.7
        }
        
        # Pattern-based analysis
        dangerous_patterns = [
            (r'eval\s*\(', 'Code injection risk via eval()'),
            (r'exec\s*\(', 'Arbitrary code execution via exec()'),
            (r'os\.system\s*\(', 'Shell command injection risk'),
            (r'subprocess\.(call|run|Popen)', 'Command injection vulnerability'),
            (r'(password|secret|key)\s*=\s*["\'][^"\']+["\']', 'Hardcoded credentials'),
            (r'SELECT\s+.*\s+FROM\s+.*WHERE.*\+', 'Potential SQL injection'),
            (r'<script\s*>', 'XSS vulnerability in HTML output'),
            (r'pickle\.loads?\s*\(', 'Deserialization vulnerability'),
        ]
        
        issues = []
        risk_factors = 0
        
        for pattern, description in dangerous_patterns:
            if re.search(pattern, code, re.IGNORECASE):
                issues.append(description)
                risk_factors += 1
        
        result["issues"] = issues
        result["risk_score"] = min(risk_factors * 0.15, 1.0)
        
        # Try to enhance with AI if available
        if self.openai_key:
            try:
                ai_result = await self.validate_with_openai(code)
                if "error" not in ai_result:
                    # Combine results
                    result["risk_score"] = max(result["risk_score"], ai_result["risk_score"])
                    result["issues"].extend(ai_result["issues"])
                    result["issues"] = list(set(result["issues"]))  # Remove duplicates
                    result["confidence"] = ai_result["confidence"]
                    result["agent"] = "Pattern Matcher + GPT-3.5"
            except:
                pass  # Fall back to pattern matching only
        
        return result

# Test the real AI integration
async def test_real_ai():
    """Test real AI validation"""
    validator = RealAIValidator()
    
    dangerous_code = """
import os
import subprocess

# Dangerous code
API_KEY = "sk-1234567890abcdef"
password = "admin123"

def process_input(user_input):
    query = f"SELECT * FROM users WHERE name = '{user_input}'"
    os.system(f"grep {user_input} /var/log/app.log")
    eval(user_input)
    return query
"""
    
    print("ðŸ§ª Testing real AI validation...")
    result = await validator.quick_validate(dangerous_code)
    print("Validation result:", json.dumps(result, indent=2))
    return result

if __name__ == "__main__":
    asyncio.run(test_real_ai())